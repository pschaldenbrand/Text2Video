# Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization


[Peter Schaldenbrand](https://pschaldenbrand.github.io/#about.html), [Zhixuan Liu](https://ariannaliu.github.io/), and [Jean Oh](https://www.cs.cmu.edu/~./jeanoh/)

The Robotics Institute, Carnegie Mellon University


<a href="https://colab.research.google.com/github/pschaldenbrand/Text2Video/blob/main/Text2Video.ipynb"><img data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667"></a>

This is an approach to generating videos based on a series of given language descriptions of the video. We currently only have a Colab implementation which is linked above.

![A ballerina frog dancing](./images/frog_ballerina.gif)

Please message Peter at pschalde at andrew dot cmu dot edu with any questions or make a GitHub issue.  Thanks!
